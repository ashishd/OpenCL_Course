{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24814a34-99bd-4235-a261-7b09473d4311",
   "metadata": {},
   "source": [
    "# Memory managment in OpenCL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85a12c9-d482-4b05-a0d5-e2e5c2e0cf6e",
   "metadata": {},
   "source": [
    "In previous lessons we have looked at straightforward ways in which memory was allocated on the host and then copied to the device for use as global memory by the kernel. In the introduction we briefly covered the five different memory spaces that are accessible to an OpenCL Program.\n",
    "\n",
    "* Host memory\n",
    "* Global memory\n",
    "* Local (shared) memory\n",
    "* Private memory\n",
    "* Constant memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c32cea-4730-4e40-99e8-933dcb20ce65",
   "metadata": {},
   "source": [
    "**Host memory** is usually the largest memory space on the host, and **global memory** is the largest and slowest memory space available on the compute device. **Local** and **Constant** memory is usually placed in the small, fast caches on the compute device. **Private memory** is usually located in the registers, which are normally the fastest and smallest memory spaces available on the compute device. A programmer has some degree of control over where memory is stored during the operation of an OpenCL program. The diagram below shows what memory is available for access by both host and kernel threads (work-items) at runtime. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62146edd-84d2-40d7-a7f4-7c1dde1e1a08",
   "metadata": {},
   "source": [
    "<figure style=\"margin-left:auto; margin-right:auto; width:80%;\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/memory_spaces.svg\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Access to memory from kernel and host threads.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99477a5-38b9-4632-94ce-2dcf8982c0d9",
   "metadata": {},
   "source": [
    "Kernel threads (work-items) can access *global*, *constant*, *local* and *private* memory, whereas host threads can only access *host* and *global* memory. Private memory for a kernel thread is exclusive to the kernel, meaning that no other kernel can access the same private memory. Local memory is accessible to all kernel threads in a workgroup, but not to kernel threads from another workgroup. *Global* and *Constant* memory is accessible from all kernel threads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6b4870-3144-4d85-9e31-caeb6ad48f4c",
   "metadata": {},
   "source": [
    "## Memory access from the host\n",
    "\n",
    "From the introduction we know that Buffers are allocated on the host and they are migrated in and out of the compute device when they are needed. Here are some ways we can create Buffers and copy memory in and out of them. \n",
    "\n",
    "### Buffer creation\n",
    "\n",
    "Thus far we have been creating Buffers with the [clCreateBuffer](https://www.khronos.org/registry/OpenCL/sdk/3.0/docs/man/html/clCreateBuffer.html) function using the **CL_MEM_READ_WRITE** flag. For example, this code creates a Buffer that has read-write access from the kernel, but no additional functionality.\n",
    "\n",
    "```C++\n",
    "    cl_mem buffer_C = clCreateBuffer(context, \n",
    "                                     CL_MEM_READ_WRITE, \n",
    "                                     nbytes_C, \n",
    "                                     NULL, \n",
    "                                     &errcode);\n",
    "```\n",
    "\n",
    "#### IO permission flags\n",
    "\n",
    "We can choose other IO flags to let the OpenCL implementation how the Buffer is to be used. This may unlock additional optimisations.\n",
    "\n",
    "| **Allocation flag** | **Functionality** | \n",
    "| :- | :- | \n",
    "|CL_MEM_READ_WRITE| Read-write access from a kernel | \n",
    "|CL_MEM_WRITE_ONLY| Write-only access from a kernel | \n",
    "|CL_MEM_READ_ONLY | Read-only access from a kernel | \n",
    "|CL_MEM_HOST_WRITE_ONLY | Write-only access from the host | \n",
    "|CL_MEM_HOST_READ_ONLY | Read-only access from the host | \n",
    "|CL_MEM_HOST_NO_ACCESS | No access from the host | \n",
    "\n",
    "Common-sense applies in the use of these flags, for example **CL_MEM_WRITE_ONLY** is incompatible with **CL_MEM_READ_WRITE** and behaviour is undefined if one tries to write to a buffer that has been set as **CL_MEM_READ_ONLY**.\n",
    "\n",
    "#### Using host memory\n",
    "\n",
    "The flag **CL_MEM_USE_HOST_PTR** allows the Buffer to use host memory as the backing store for a Buffer. One must make sure that there is enough host memory allocated to cover the memory used by the buffer and that the host memory is not de-allocated while the buffer is using it. OpenCL implementations are free to allocate caches on the compute device for temporary usage and then synchronize as required. Memory synchronization can be explicitly done using **mapping**, which will be discussed shortly.\n",
    "\n",
    "Similarly, the flag **CL_MEM_COPY_HOST_PTR** creates an OpenCL buffer but copies memory from a host pointer during buffer creation. The host pointer is then released.  \n",
    "\n",
    "Both **CL_MEM_USE_HOST_PTR** and **CL_MEM_COPY_HOST_PTR** require a host pointer to be passed into the call to [clCreateBuffer](https://www.khronos.org/registry/OpenCL/sdk/3.0/docs/man/html/clCreateBuffer.html).\n",
    "\n",
    "#### Creating buffers for asynchronous copies\n",
    "\n",
    "Pinned memory is host memory that cannot be paged out to swap. It enables fast Direct Memory Access (DMA) transfers from the host, however it is limited by the OS to a fraction of the available memory. Normally, transfers between host and device - for example using [clEnqueueReadBuffer](https://www.khronos.org/registry/OpenCL/sdk/3.0/docs/man/html/clEnqueueReadBuffer.html) or [clEnqueueWriteBuffer](https://www.khronos.org/registry/OpenCL/sdk/3.0/docs/man/html/clEnqueueReadBuffer.html) - are synchronous, meaning they actually block until the transfer completes. The flag **CL_MEM_ALLOC_HOST_PTR** allocates pinned memory on the host as the backing store for the OpenCL buffer. This also enables asynchronous transfers so that IO movement can occur at the same time as compute.\n",
    "\n",
    "### Explicit memory movement\n",
    "\n",
    "#### Rectangular copies\n",
    "\n",
    "#### Mapping\n",
    "\n",
    "### Shared memory and synchronisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d4816a-e65e-4186-9ee3-4c8745859a45",
   "metadata": {},
   "source": [
    "## Memory access from kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d6193d-622d-474e-94af-fd5e6ce81e9e",
   "metadata": {},
   "source": [
    "### Global"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c411c212-c335-412c-bbad-f6647885d858",
   "metadata": {},
   "source": [
    "### Local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3139b0c-ce16-47d1-b95b-2037f8a9f983",
   "metadata": {},
   "source": [
    "### Shared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f155b3-0ede-4b9e-be9c-7dff37909516",
   "metadata": {},
   "source": [
    "### Private"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3962512-af4a-4530-b65d-28ce213b8a14",
   "metadata": {},
   "source": [
    "### Accessing vector elements "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f86af5e-1857-42e8-a3ff-7036697fe3bf",
   "metadata": {},
   "source": [
    "## Using shared memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306002ca-19a5-4d65-8823-0d605a40eea4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
