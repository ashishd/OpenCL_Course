{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e5ce07f-dc2b-4014-9e4f-e257e897cc75",
   "metadata": {},
   "source": [
    "# Optimising OpenCL applications\n",
    "\n",
    "Optimal performance for OpenCL applications is a complex multi-dimensional problem. It depends not only on peak utilisation of compute units, but also the timeliness and speed that memory is passed in and out of compute units. It is a general rule that vendor-specific libraries are the best way to achieve optimal compute performance, however for OpenCL these libraries are often not available. At the same time OpenCL implementations and their underlying hardware have smart technologies to optimise processing for commonly used computing operations, and sometimes the simplest approach works best. At other times better compute performance comes at a significant increase in complexity of the program. CPU's and GPU's architectures are optimsed for different ways of computing. This means that a kernel that performs well on a CPU might perform poorly on a GPU, and vice-versa. Experimentation is the key. In the following sections we will try a number of experiments and see what kind of difference they make to the matrix multiplication problem.\n",
    "\n",
    "## Fundamentals of CPU and GPU architectures\n",
    "\n",
    "It is helpful to first cover the fundamentals of computer architectures and explore differences between CPU's and GPU's. \n",
    "\n",
    "### Hardware threads and processing elements\n",
    "\n",
    "From the introduction we have seen that that a processor is partioned into cores and each core provides a number of hardware threads that become the processing elements for an OpenCL kernel. \n",
    "\n",
    "<figure style=\"margin-bottom 3em; margin-top: 2em; margin-left:auto; margin-right:auto; width:60%\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/Intel-8th-Gen Core-3.jpg\"> <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Figure: A processor die showing an eighth-generation <span>Intel&trade;</span> <span>Core&trade;</span> processor. Image used with permission courtesy of Intel Corporation. </figcaption>\n",
    "</figure>\n",
    "\n",
    "Cores have a clock cycle and can perform a limited number of instructions per cycle. During a computation instructions such as math commands are executed on the hardware threads, using memory that is passed in and out of the core from a hierarchy of caches. If this memory does not arrive on time, or get disposed of on time, then the hardware threads will stall and peak theoretical performance will not be acheived."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97402496-647a-4e9e-ab6e-df6ae50d324d",
   "metadata": {},
   "source": [
    "### Cache topics\n",
    "\n",
    "#### Hierarchy\n",
    "\n",
    "The fastest memory in a processor (CPU or GPU) are the registers. They operate at or near the same clock speed as the CPU and are located on the CPU die. This memory is not cheap however, and memory for a computation is shuffled to and from the processor through a hierarchy of caches with the following sizes and locations:\n",
    "\n",
    "* Register, fastest memory (few kB)\n",
    "* L1 cache, fast on-die memory (tens of kB)\n",
    "* L2 cache, fast on-die memory (hundreds of kB)\n",
    "* L3 cache, fast memory near the device or on-die. (few MB to tens of MB)\n",
    "* Global or device memory (RAM, few GB onwards)\n",
    "\n",
    "The most used memory is stored in the lowest level caches. Memory that is not used as often is evicted to the slower caches as necessary. Memory that is rarely at all is flushed out to global memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e48ad5-9b65-4788-b45d-4dbc737b8889",
   "metadata": {},
   "source": [
    "#### Cache lines - the fundamental unit of memory transactions\n",
    "\n",
    "Memory passed from caches doesn't arrive at the CPU in units of individual bytes, instead it arrives in transactional units called cache lines. Cache lines are around 64-128 bytes, which can store 16-32 floating point numbers.\n",
    "\n",
    "<figure style=\"margin-bottom 3em; margin-top: 2em; margin-left:auto; margin-right:auto; width:60%\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/cache_line.svg\"> <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Figure: A cache line 64 bytes wide. </figcaption>\n",
    "</figure>\n",
    "\n",
    "This means that if a processor needs to fetch from or store a value in memory, then the cache hierarchy must transport the **entire cache line** in which the value is located. Caches have slots in them where cache lines can be stored, however if a cache line is not in the faster caches, then it must be fetched from main memory at the cost of wasted processor cycles. \n",
    "\n",
    "> A key to achieving good performance with memory is to then **try and use as many neighbouring elements in cache line as possible** by subsequently fetching and storing memory that is in the immediate \"neighbourhood\" of the initial memory access. \n",
    "\n",
    "In the context of OpenCL it is advantageous if a work-item or work-items in a workgroup access neighbouring memory locations. Then memory transfers can be combined in what is known as **coalesced memory access**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf5fe13-d3a5-4e39-af39-f5cf033270a0",
   "metadata": {},
   "source": [
    "#### Latency\n",
    "\n",
    "Latency is the number of cycles a processing element has to wait for memory to arrive from a cache. here are some indicative numbers on latency for CPU and GPU caches.\n",
    "\n",
    "| Cache | memory latency (clock cycles on a CPU) | memory latency (clock cycles on a GPU)\n",
    "|:----:|-----:|-----:|\n",
    "|   Register  | ~1 | ~20 |\n",
    "|   L1  | ~4 | ~30-100|\n",
    "| L2 | ~12 | ~175-300 |\n",
    "| L3 | ~24-32 | NA |\n",
    "| Device memory | >= 100 | 300-800 | \n",
    "\n",
    "Sources for these numbers: [GPU numbers 1](http://lpgpu.org/wp/wp-content/uploads/2013/05/poster_andresch_acaces2014.pdf), [GPU numbers 2](https://arxiv.org/pdf/1804.06826.pdf), [CPU numbers](http://techreport.com/review/27018/intel-xeon-e5-2687w-v3-processor-reviewed/4)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cc6577-c916-460e-b997-53667bafac7e",
   "metadata": {},
   "source": [
    "#### Throughput\n",
    "\n",
    "Throughput is a measure of how fast memory can be transported from a cache to the processing element. As follows is a rough guide on what one can expect to see from CPU and GPU cache throughput.\n",
    "\n",
    "| Cache | speed (GB/s on a CPU) | speed (GB/s on a GPU) |\n",
    "|:----:|-----:|-----:|\n",
    "|   L1  | 2000-4000 | ~1000+ |\n",
    "| L2 | 1000-1500 | ~1000 |\n",
    "| L3 | 500-1000 |NA|\n",
    "| Device memory | 16 - 128 | 100-1200 |\n",
    "\n",
    "Sources for these numbers: [CPU numbers](http://www.tested.com/tech/457440-theoretical-vs-actual-bandwidth-pci-express-and-thunderbolt), [GPU numbers](http://meseec.ce.rit.edu/551-projects/spring2015/3-2.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9b6818-3486-4334-9fd1-987af13078a6",
   "metadata": {},
   "source": [
    "#### Thoughts on latency and throughput with OpenCL kernels in mind\n",
    "\n",
    "With either CPU or GPU processors we must make sure to make best use of cache lines. This means creating workgroups whose neighbouring work items also work on neighbouring elements of a memory allocation. It seems that throughput from global memory is significant faster on a GPU device, however memory will take more cycles to arrive. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65100afd-ec74-4403-bb6d-a6d0cdc6f569",
   "metadata": {},
   "source": [
    "### Floating point operations\n",
    "\n",
    "Math operations (such as multiply or add) on floating point numbers are an essential component of scientific applications. These operations are typically performed on 32- or 64-bit numbers, however 16-bit operations are becoming popular in machine learning as those do not require extremely precise calculations. Performance of a processor is measured in FLOPS, or the total number of floating point operations its compute units can perform in one second. At the time of writing units of gigaFLOPS ($10^{9}$ FLOPS) or teraFLOPS ($10^{12}$ FLOPS) are commonly used to describe compute device performance.\n",
    "\n",
    "The raw floating point performance of a compute device is very much dependent on precision and wether or not the architecture is built for it. Gaming hardware is often optimised for 32-bit floats, and 64-bit processing may be emulated in software or accomplished with fewer cores. For example the RTX3060 graphics card has a 64-bit floating point processing rate that is **64 times lower** than 32-bit due to 64 times fewer 64-bit capable processing elements. CPU's have SIMD processing units that can process vectors of floats with a single instruction. At the time of writing vectors of up to 512 bits (16 floats) can be processed by AVX-512 instructions.\n",
    "\n",
    "Compute performance within a kernel is also determined by the type of math instruction performed. \n",
    "> Addition, multiplication, and fused multiply-add are among the **cheapest** operations to perform (costing a few processor cycles), whereas division, square-root, and trigonometric functions are typically an **order of magnitude** more expensive. Writing your kernels to minimise expensive math operations will help, but waiting for memory is time consuming, and during those times kernel math can often be done for free."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecbe70e-7678-4891-a019-87d817cc0f76",
   "metadata": {},
   "source": [
    "### CPU architecture specifics\n",
    "\n",
    "CPU's usually have fewer than 100 cores, but each core has sophisticated instruction handling and nice things like pre-emptive memory fetching and branch prediction. One can think of CPU cores as smart workers, and the cache latency timings above means they are more nimble than GPU's. CPU's have registers, and on-die L1 and L2 caches. The L3 cache is usually also on-die, but is in an area that is commonly accessible to other cores. In <span>AMD&trade; Zen&trade; 3+</span>  CPU's the cores are grouped in chiplets of 8 cores called core complexes (CCX's), and the L3 cache is located in an area of the chiplet with access from each core. \n",
    "\n",
    "<figure style=\"margin-bottom 3em; margin-top: 2em; margin-left:auto; margin-right:auto; width:80%\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/cpu_cores.svg\"> <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Figure: Section of a CPU processor, showing cores with floating point SIMD units and integer pipelines. </figcaption>\n",
    "</figure>\n",
    "\n",
    "With OpenCL we might be able to use [clCreateSubDevices](https://www.khronos.org/registry/OpenCL/sdk/3.0/docs/man/html/clCreateSubDevices.html) to partition a CPU into sub-devices, so that workgroups can run on a single chiplet. In terms of math operations, CPU's have integer processing and SIMD (Single Instruction Multiple Data) units that can process vectors of floating point numbers. At the time of writing vectors of up to 8-16 floats (AVX2-AVX512) may be processed at a time using the SIMD units of a CPU. It depends largely on what kind of vector instructions are supported by the chipset.\n",
    "\n",
    "> Getting good floating point performance from a CPU is **critically** dependent on wether or not one can effectively leverage the SIMD units of the CPU. Using **vectors** in OpenCL kernel is a way to provide strong hints the to implementation's device compiler to emit vector instructions that work on the CPU.\n",
    "\n",
    "#### Example specifications\n",
    "\n",
    "Here are some of the top CPU's at the moment and their specs.\n",
    "\n",
    "| CPU | Base clock frequency (GHz) | Cores | Hardware threads | L1 Cache (KB) | L2 Cache (KB) | L3 cache (MB) | FP SIMD width (bits) | Tflops (FP32 calculated) |\n",
    "|:----:|-----:| -----: | -----: | :----: | :----: | :----: | :----: | :----: |\n",
    "| Intel Xeon Platinum 8380HL | 2.9 | 28 | 56 | 28x64 | 28x1024 | 38.5 | 512 | 1.3 |\n",
    "| AMD EPYC 7H12 | 2.6 | 64 | 128 | 64x32 | 64x512 | 16x16 | 256 | 1.3 |\n",
    "\n",
    "Source for the CPU specs [Xeon](http://www.cpu-world.com/CPUs/Xeon/Intel-Xeon%20E7-4850%20v4.html) and [Epyc](https://en.wikipedia.org/wiki/Epyc).\n",
    "\n",
    "### GPU architecture specifics\n",
    "\n",
    "There is some confusing terminology surrounding the definition of a processor core in GPU's. GPU's also have processor cores, called **Streaming Multiprocessors** in NVIDIA terminology, **Compute Units** in AMD terminology, and **Xe-Cores** in Intel terminology. Each of these cores schedule instructions and move data in the caches. Instructions are executed in lock-step, over teams of hardware threads. Data for each hardware thread lives in the registers and is manipulated by individual SIMD-like processing elements with floating point capabilities. The terminology for thread teams is called SIMT (Single Instruction Multiple Thread), and individual elements are called **CUDA Cores** in NVIDIA terminology, and **Shader cores** in AMD terminology. The teams are known as **Warps** in NVIDIA terminology and **Wavefronts** in AMD. Teams are 32-64 work items large on NVIDIA hardware, and 64 work items large on AMD hardware.  It appears that Intel have adopted a hybrid approach in using both SIMT and SIMD in their vector units, so using vector instructions might be critical in unlocking performance with OpenCL.\n",
    "\n",
    "<figure style=\"margin-bottom 3em; margin-top: 2em; margin-left:auto; margin-right:auto; width:100%\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/gpu_cores.svg\"> <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Figure: Floating point units (FP32), registers, and caches for different GPU's. </figcaption>\n",
    "</figure>\n",
    "\n",
    "In a GPU there are several cores, and this means that there are thousands of processing elements available to run a kernel on.\n",
    "\n",
    "> There are a few things to note with the SIMT approaches of NVIDIA and AMD. First of which is that instructions are executed in lock-step. This means that the **ideal workgroup size is going to be a multiple of the work-items in a warp or wavefront**. You can get the the preferred workgroup size using **clinfo** or querying for the **CL_KERNEL_PREFERRED_WORK_GROUP_SIZE_MULTIPLE** attribute for a compiled kernel with [clGetKernelWorkGroupInfo](https://www.khronos.org/registry/OpenCL/sdk/3.0/docs/man/html/clGetKernelWorkGroupInfo.html). Secondly, branching (using if statements in kernels with multiple execution paths) can be costly, as the whole team must execute all branches sequentially regardless of wether or not work is done. Next, we note there are **many** times more hardware threads available with GPU's, so you need to schedule enough work-items to keep those threads busy. This is known as **occupancy**. Finally, it is important to note that it is the processor core (Streaming Multiprocessor, Compute Unit, or Xe core) that schedules instructions, the individual processing elements are by design not as sophisicated as their CPU counterparts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6915f102-5f20-4fd6-b58e-ba0b5b29e368",
   "metadata": {},
   "source": [
    "#### Matrix and tensor cores not available in OpenCL\n",
    "\n",
    "Up to this point I have completely skipped over the extra matrix (AMD, INTEL) and tensor (NVIDIA) core functionality present in modern GPU's. This special matrix processing machinery has applications in machine learning. Unfortunately this ability is not present in OpenCL and at the time of writing can only be unlocked through vendor implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca97a2b1-9178-49b7-9ec2-5da16e989e83",
   "metadata": {},
   "source": [
    "#### Example GPU specifications\n",
    "\n",
    "Here are some specifications on more recent GPU's. AMD has a more SIMD-like approach to computing floats. Therefore they don't have their own dedicated 64-bit processing elements. Instead 64 bit processing is either emulated or running at half that of 32-bit floats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a23f4e-4c1e-486f-9abb-db812fdcd2a8",
   "metadata": {},
   "source": [
    "| Card | Boost clock (GHz)| Compute Units | FP32 Processing Elements | FP64 Processing Elements (equivalent compute capacity) | L1 Cache (KB) | L2 Cache (KB) | device memory (GB) | Peak Tflops (FP32)| Peak Tflops (FP64)|\n",
    "|:----:|:-----| :----- | :----- | :---- | :---- | :---- | :---- | :---- | :---- |\n",
    "| NVIDIA Tesla K40 | 0.875 | 15 | 2880 | 960 | 48 | 1536 | 12 |5.04|1.68|\n",
    "| NVIDIA Tesla M60 |1.178 |32| 4096| NA | 32x2x48 | 3072 | 8 |4.8|0.151|\n",
    "| NVIDIA Tesla P100 |1.480| 56 | 3584 | 1792 | 56x64| 4096 | 16 |10.6|5.3|\n",
    "| NVIDIA Tesla V100 |1.530| 80 | 5120 | 2560 | 80x96 | 6144 | 16 |15.7|7.8|\n",
    "| NVIDIA Tesla A100 |1.410| 108 | 6912 | 3456 | 108x164 | 40960 | 40 |19.5|9.7|\n",
    "| AMD Radeon Instinct MI6 |1.233| 36 | 2304 |  | 36x16 | 2000 | 16 |5.73|0.358|\n",
    "| AMD Radeon Instinct MI8 |1.000| 64 | 4096 |  | 64x16 | 2000 | 4 |8.2|0.512|\n",
    "| AMD Radeon Instinct MI25 |1.501 | 64 | 4096 |  | 64x16 | 4000 | 16 |12.3 |0.768|\n",
    "| AMD Radeon Instinct MI50 |1.746 | 60 | 3840 |  | 60x16 | 4000 | 16 |13.4 |6.7|\n",
    "| AMD Radeon Instinct MI100 |1.502 | 120 | 7680 |  | 120x16 | 8000 | 32|23.1 |11.5|\n",
    "| AMD Radeon Instinct MI200 |1.7 | 208 | 13312 |  | 208x16 | 16000 | 128 | 90.5 | 45.3 |\n",
    "| AMD Radeon Instinct MI250x |1.7 | 220 | 14080 |  | 220x16 | 16000 | 128 | 95.7 | 47.9 |\n",
    "\n",
    "Source for these numbers [Acceleware](https://training.acceleware.com/blog/Tesla-Meets-Maxwell), [NVIDIA](https://devblogs.nvidia.com/inside-volta/), [AMD](https://www.amd.com/en/graphics/servers-radeon-instinct-mi), [Microway](https://www.microway.com/knowledge-center-articles/in-depth-comparison-of-nvidia-tesla-maxwell-gpu-accelerators/), and [TechPowerup](https://www.techpowerup.com/gpudb/2760/tesla-m60)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e666b25d-e043-43ba-b640-707df5d25864",
   "metadata": {},
   "source": [
    "## Optimising the use of your compute devices\n",
    "\n",
    "With these principles in mind we can go about exploring ways to improve performance on the matrix multiplication problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e197a452-fe28-47c5-904e-de0394c44229",
   "metadata": {},
   "source": [
    "### Arithmetic intensity and working out where to spend the effort\n",
    "\n",
    "The **Arithmetic intensity** is the ratio of FLOPS computed per Byte transferred to the processing element. It helps us determine if our algorithm is likely to be constrained by bandwidth or floating point performance. For our problem matrix A is of size ($N0_{C}, N1_{A}$) and B is of size ($N1_{A}, N1_{C}$), every element computed in matrix C requires $N1_A$ loads from A, $N1_A$ loads from B, and 1 store to C. It also requires $N1_A$ multiplications and $N1_A$ additions. The arithmetic intensity is then\n",
    "\n",
    "$$ FB = \\frac{2N1_A}{(2N1_A+1)b} $$\n",
    "\n",
    "where **b** is the number of bytes stored per element. When $N1_A$ is large the **FB** ratio for matrix multiplication is then\n",
    "\n",
    "$$ FB \\approx \\frac{1}{b} $$\n",
    "\n",
    "If a processor has a peak performance of $P_{\\mbox{max}}$ FLOPS and a peak bandwidth of $\\beta_{\\mbox{max}}$ bytes per second, then the problem will be constrained by the device's memory bandwidth if $FB < \\frac{P_{\\mbox{max}}}{\\beta_{\\mbox{max}}}$. For example, the AMD Mi250x GPU has a peak 32-bit floating point rate of 95.7 TFLOPS and a peak memory bandwidth of 3.2 TB/s. Problems will be constrained by memory bandwidth up to an arithmetic intensity of \n",
    "\n",
    "$$FB_{max}=\\frac{95.7}{3.2} \\approx 30$$\n",
    "\n",
    "for more information see the [Wikipedia Article on Roofline Models](https://en.wikipedia.org/wiki/Roofline_model).\n",
    "\n",
    "Usually the critical **$FB_{\\mbox{max}}$** ratio for processors is much larger than $\\frac{1}{b} $. What this means for the matrix multiplication problem is the following:\n",
    "\n",
    "* Usually are not going to get anywhere the rated peak performance of the compute device.\n",
    "* Optimising memory transfers is where we should concentrate our efforts.\n",
    "* We can likely get most of compute operations done during memory transfers.\n",
    "* Speedups over a CPU implementation are more likely to be limited to the difference in memory bandwidth speeds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97421563-1dd4-4340-b76c-868b76a6fe5f",
   "metadata": {},
   "source": [
    "### Data precision\n",
    "\n",
    "In scientific computing the precision and range of 64-bit arithmetic is important, for example in the solution of sensitive differential equations. Compute hardware is often optimised for 32-bit floats though, and if your algorithm supports it you can gain an optimisation by running at lower precision. In the example below we run a matrix multiplication problem at 64-bit and 32-bit precision to see how much of a difference it makes to kernel execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcced67c-c103-4ec3-b21e-e83a00f313de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.insert(0, os.path.abspath(\"../include\"))\n",
    "\n",
    "import py_helper\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "NCOLS_A = 1025\n",
    "NROWS_C = 1025\n",
    "NCOLS_C = 1025\n",
    "\n",
    "# A is of size (NROWS_C, NCOLS_A)\n",
    "# B is of size (NCOLS_A, NCOLS_C)    \n",
    "# C is of size (NROWS_C, NCOLS_C)\n",
    "\n",
    "mat_mul_double=py_helper.MatMul(NCOLS_A, NROWS_C, NCOLS_C, np.float64)\n",
    "mat_mul_float=py_helper.MatMul(NCOLS_A, NROWS_C, NCOLS_C, np.float32)\n",
    "\n",
    "# Make up objects for timing results and local optimisation\n",
    "timings=py_helper.TimingResults()\n",
    "local_opt=py_helper.LocalOpt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d062a6-d449-4d8a-baae-cfb60c48a9e5",
   "metadata": {},
   "source": [
    "#### Matrix multiplication with 64-bit precision on both CPU and GPU implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9963f007-f8d0-42ad-bb65-48e901deb225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Nothing to be done for 'all'.\n",
      "\t               name: AMD Ryzen Threadripper 2950X 16-Core Processor  \n",
      "\t global memory size: 134949 MB\n",
      "\t    max buffer size: 33737 MB\n",
      "\t     max local size: (8192,8192,8192)\n",
      "\t     max work-items: 8192\n",
      "returncode is 0\n",
      "Min time is 16.648 ms, at the local size of (32,8,1).\n"
     ]
    }
   ],
   "source": [
    "!make;\n",
    "mat_mul_double.make_data()\n",
    "timings.add_result(\n",
    "    local_opt.run_problem([\"./mat_mult_double.exe\",\"-cpu\"], plot=False),\n",
    "    \"Double precision (CPU)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb46a4b2-cd92-4082-990d-4a65871b7feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Nothing to be done for 'all'.\n"
     ]
    }
   ],
   "source": [
    "!make;\n",
    "timings.add_result(\n",
    "    local_opt.run_problem([\"./mat_mult_double.exe\",\"-gpu\"], plot=False),\n",
    "    \"Double precision (GPU)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb57dc7-489c-43e2-8d5c-571de8876fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_mul_double.check_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f847e6-e259-4baf-879b-ac87c74e500e",
   "metadata": {},
   "source": [
    "#### Matrix multiplication again with single precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26152c7f-2eba-42ef-8ef8-2aa9a4fbfd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "!make;\n",
    "mat_mul_float.make_data()\n",
    "timings.add_result(\n",
    "    local_opt.run_problem([\"./mat_mult_float.exe\",\"-cpu\"], plot=False),\n",
    "    \"Single precision (CPU)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838c4cdc-583b-4b79-a054-90428035ec59",
   "metadata": {},
   "outputs": [],
   "source": [
    "timings.add_result(\n",
    "    local_opt.run_problem([\"./mat_mult_float.exe\",\"-gpu\"], plot=False),\n",
    "    \"Single precision (GPU)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775d0148-fde2-4068-881b-676b68445a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_mul_float.check_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c869b567-e8b1-4705-b835-e82bc03cc6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "timings.plot_results(\"Single precision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629f5162-b5ff-4f5f-b84e-2c5731be139d",
   "metadata": {},
   "source": [
    "#### Conclusions from precision\n",
    "\n",
    "The single precision results show that using GPU's with 32-bit numbers is clearly a perfomance advantage over 64-bit. While the GPU peak processing rate for 32-bit numbers is many times greater than for 64-bit, the fact that the algorithm is memory bandwidth limited constrains the performance advantage of 32-bit processing to around **6x** over the single precision CPU implementation (appoximately the difference in memory bandwidth), and around **4x** over the double precision GPU implementation.\n",
    "\n",
    "It appears that on the CPU the difference between single precision and double precision is not significant. This is probably because the processor is designed for 64 bit computing and SIMD vector instructions have not been enabled. Since caches have lower latency on a CPU, the difference in data size being transferred is not significant to effect a performance difference. For GPU's, the 32-bit processing rate is faster, and the data transfer is also halved. This contributes to a 2-4x speedup over the double precision GPU implementation and a 6x speedup over the CPU implementation. \n",
    "\n",
    "Unless your algorithm requires high precision then using the lowest precision possible can yields a performance improvement with GPU's due to lower amounts of memory transfer and higher processing rates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01a9576-666e-42a4-ac31-a0efc6442da0",
   "metadata": {},
   "source": [
    "### Occupancy\n",
    "\n",
    "An active workgroup is one whose work items have started executing. GPU cores best when there are several active workgroups on a core at the same time. This is so latencies (such as fetching from global memory) can be hidden behind execution. Cores have a limit to the number of active workgroups and best performance is achieved when this limit is attained, so-called 100% occupancy.\n",
    "\n",
    "Some things affect occupancy: \n",
    "\n",
    "* If kernels use **too much private memory** then available registers will be depleted, and this limits the number of active workgroups that can be scheduled.\n",
    "* If kernels use **too much local memory** then it can either reduce occupancy or else local memory spills to global memory at the cost of greater latency.\n",
    "* If the workload in kernels is **uneven**, (some kernels take a lot longer than others to complete), then occupancy can be curtailed.\n",
    "* If there simply isn't enough work scheduled.\n",
    "\n",
    "Some tips for maximising occupancy are then.\n",
    "\n",
    "* Keep the number of private variables low\n",
    "* Keep the shared memory size low\n",
    "* Keep the workload consistent across workgroups\n",
    "* Use the recommended workgroup size for each kernel. You can get this number by querying for the **CL_KERNEL_PREFERRED_WORK_GROUP_SIZE_MULTIPLE** attribute for a compiled kernel with [clGetKernelWorkGroupInfo](https://www.khronos.org/registry/OpenCL/sdk/3.0/docs/man/html/clGetKernelWorkGroupInfo.html). \n",
    "* Every core may have a finite number of workgroups active at any one time. If possible schedule enough workgroups so that every core in the GPU is running with this number of active workgroups.\n",
    "\n",
    "#### Example\n",
    "\n",
    "For the matrix multiplication algorithm as it stands, we are using a small number of private variables and we have one work item for every element in matrix C. For a matrix size of $1025^{2}$ there can be just over $1025^2 = 1,050,625$ work items. From the occupancy table in NVIDIA a NVIDIA RTX 3060 with CUDA compute capability 8.6 can have 1,536 threads per streaming multiprocessor active at any one time. Since there are 28 streaming multiprocessors then 28x1536 = 43,008 threads may be active at any one time. Therefore we know there is more than enough work scheduled to keep the streaming multiprocessors busy. In the default matrix multiplication algorithm there is no local memory usage, so we anticipate that the GPU can sustain full occupancy.\n",
    "\n",
    "### Local size optimisation and coalesced memory access\n",
    "\n",
    "Performance is a complicated function that is dependent on the performance of the processor and how efficiently cache lines are being used. Local size is a way to control how the compute device accesses memory. In <a href=\"../include/cl_helper.hpp\">cl_helper.hpp</a> is a function called **h_optimise_local** that performs a brute force search over a set of input local size experiments. The function **run_problem** in the Python class **LocalOpt2D** writes a file called **input_local.dat** which contains the experiments. Then during execution **h_optimise_local** reads this file and gathers timing statistics for each experiment. Timing results are written to **output_local.dat** and the **run_problem** reads the results and finds the best performing time. Now we apply this code to find the best performing local size for the basic matrix multiplication algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decc994a-19d3-4aa2-975b-2d6d500efaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "timings.add_result(\n",
    "    local_opt.run_problem([\"./mat_mult_float.exe\",\"-gpu\"], plot=True),\n",
    "    \"Single precision (GPU)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9759c8-b966-46c4-9c43-189235bd4d16",
   "metadata": {},
   "source": [
    "From the graph it seems that the best performing local size is one where there is an elongated number of work items along the dimension of contiguous memory access. Efficient cache usage is taking place along the rows of **A** but it may seem counterintuitive for memory access along the columns of **B** until we overlay the cache lines. Then we see that as we loop along the columns of **B**, cache lines are being reused by their neighbouring work items along dimension 1.\n",
    "\n",
    "<figure style=\"margin-left:auto; margin-right:auto; width:100%;\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/coalesced_memory_access.svg\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Rectangular memory copy.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6197b5ec-977f-47b5-9f7c-47435c7b5ee5",
   "metadata": {},
   "source": [
    "### Prefetching\n",
    "\n",
    "Within OpenCL kernels there is a command called **prefetch** which can prefetch global memory that is going to be used by a work-item. In the kernel **mat_mult_prefetch** in [kernels_mat_mult.c](kernels_mat_mult.c) we employ a prefetch for row **i0** of matrix A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a203b50a-057a-49e6-8754-3cbcd3f02c86",
   "metadata": {},
   "source": [
    "```C++\n",
    "    // Guard mechanism to make sure we do not go\n",
    "    // outside the boundaries of matrix C \n",
    "    if ((i0<N0_C) && (i1<N1_C)) {\n",
    "        \n",
    "        // Implement prefetching for A\n",
    "        __global float* A_i0 = &A[i0*N1_A];\n",
    "        __global float* B_i1 = &B[i1];\n",
    "        prefetch(A_i0, (size_t)N1_A);\n",
    "    \n",
    "        // Loop over columns of A and rows of B \n",
    "        for (size_t n=0; n<N1_A; n++) {\n",
    "            \n",
    "            // A is of size (N0_C, N1_A)\n",
    "            // B is of size (N1_A, N1_C)\n",
    "            \n",
    "            // Loop across row i0 of A\n",
    "            // and down column i1 of B\n",
    "            //temp+=A[i0*N1_A+n]*B[n*N1_C+i1];\n",
    "            temp += A_i0[n]*B_i1[n*N1_C];\n",
    "        } \n",
    "        // Number of rows in C is same as number of rows in A\n",
    "        C[i0*N1_C+i1]=temp;\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0ba1d8-6ad5-4082-8f79-5ff1eddf4a62",
   "metadata": {},
   "source": [
    "Try this out with the kernel timing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7947eca4-91b0-4c9b-89db-f95c2ba044f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "timings.add_result(\n",
    "    local_opt.run_problem([\"./mat_mult_prefetch.exe\",\"-gpu\"], plot=False),\n",
    "    \"Prefetch on A (GPU)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14719665-d9b8-46fe-a1cd-8795601b0e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "timings.plot_results(\"Prefetch on A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42111a2d-717b-4939-adb4-2e131336a3e4",
   "metadata": {},
   "source": [
    "From the result above we can see that prefetching makes a slight difference. Nevertheless it still might be useful for applications where you know the memory that will be needed in the cache for a work item."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a6af99-0dcf-401f-b6a6-e989144111ff",
   "metadata": {},
   "source": [
    "### Using constant memory\n",
    "\n",
    "We saw in the [Memory Management](../L6_Memory_Management/Memory.ipynb) module memory in the **__constant** address space is stored in a fast cache on the compute device. This storage space is small, on the order of a few tens to a few hundred kilobytes in size. Constant memory is a good storage place for filter coefficients, such as is used for finite difference and convolution operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab6f0ee-74a5-4535-854e-69fd86b736e9",
   "metadata": {},
   "source": [
    "### Using local memory\n",
    "\n",
    "In the <a href=\"../L6_Memory_Management/Memory.ipynp\">Memory Management Lesson</a> we implemented a matrix multiplication implementation that copies all the rows and columns for a workgroup into local memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dc743e-8a34-4be1-815b-12506622ac30",
   "metadata": {},
   "source": [
    "<figure style=\"margin-left:auto; margin-right:auto; width:100%;\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/matrix_multiplication_shmem.svg\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Using local memory in matrix multiplication.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106aea78-2981-4319-861b-4e7136d46216",
   "metadata": {},
   "source": [
    "If we try to find the optimal local size it ends in disaster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adc52f1-ae98-4b11-b38a-807f3a6b4c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "timings.add_result(\n",
    "    local_opt.run_problem([\"./mat_mult_local.exe\",\"-cpu\"], plot=False),\n",
    "    \"Local memory (CPU)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9b0d76-ceb4-4e98-a163-8f57b750ea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "timings.add_result(\n",
    "    local_opt.run_problem([\"./mat_mult_local.exe\",\"-gpu\"], plot=True),\n",
    "    \"Local memory (GPU)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb508a8f-f73d-41de-aaf5-5f410ab3ef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "timings.plot_results(\"Local memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73392fe9-b869-4862-a51f-eecdbbc52555",
   "metadata": {},
   "source": [
    "These performance results are disappointing. Due to the size of local memory required, only a small subset of the available local sizes are supported, and we aren't getting anywhere near the same performance. When using local memory one has to be careful of allocation size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0f87ba-0cfd-4b2e-bae3-b21b9c7b5087",
   "metadata": {},
   "source": [
    "### Transposing memory\n",
    "\n",
    "We saw in the local size optimisation section that coalesced memory access is still possible with accesses to matrix B, however is that memory access more or less optimal than that of A? \n",
    "\n",
    "#### Transposing B\n",
    "\n",
    "We might be able to leverage greater performance by first transposing matrix **B** to make **BT**, so that memory access occurs along the rows of **A** and **BT**.\n",
    "\n",
    "<figure style=\"margin-left:auto; margin-right:auto; width:100%;\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/matrix_multiplication_transpose.svg\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Transposed matrix multiplication.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a2ca77-ed66-4c40-872c-36c380152e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "timings.add_result(\n",
    "    local_opt.run_problem([\"./mat_mult_transpose_B.exe\",\"-gpu\"], plot=False),\n",
    "    \"Transpose B (GPU)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b1cb5f-5ec1-4df0-b0d6-e8390220d134",
   "metadata": {},
   "outputs": [],
   "source": [
    "timings.add_result(\n",
    "    local_opt.run_problem([\"./mat_mult_transpose_B.exe\",\"-cpu\"], plot=False),\n",
    "    \"Transpose B (CPU)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90414584-8f07-4452-bd5c-42afd824d881",
   "metadata": {},
   "outputs": [],
   "source": [
    "timings.plot_results(\"Transpose\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7660c68d-5e47-4d36-9ed6-199c42d0a4f2",
   "metadata": {},
   "source": [
    "By transposing matrix B we see performance that is consistently **worse** than the simple matrix multiplication approach. This is possibly due to multiple workgroups requesting the same cache line. \n",
    "\n",
    "#### Transposing A\n",
    "\n",
    "Let's try transposing A instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468fe80b-04b8-4456-8388-0ea0e052f49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "timings.add_result(\n",
    "    local_opt.run_problem([\"./mat_mult_transpose_A.exe\",\"-cpu\"], plot=False),\n",
    "    \"Transpose A (CPU)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61b04cc-5cf7-4f4b-be04-fa3cdd2ad2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "timings.add_result(\n",
    "    local_opt.run_problem([\"./mat_mult_transpose_A.exe\",\"-gpu\"], plot=True),\n",
    "    \"Transpose A (GPU)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8978022-6b60-4b5b-b828-d6849aad3ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "timings.plot_results(\"Transpose\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8198cc84-5e95-41a6-94ef-feacb2cb8efa",
   "metadata": {},
   "source": [
    "If we tranpose **A** instead we get **much better performance on GPU's** but still lesser performance on CPU's. In this instance the tranpose A algorithm is GPU-specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd474092-b041-4de2-a570-1504c08836b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "timings.add_result(\n",
    "    local_opt.run_problem([\"./mat_mult_transpose_A_local.exe\",\"-gpu\"], plot=True),\n",
    "    \"Transpose A local (GPU)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdeb0c0-ca16-44c8-8137-bd8b38c1b33c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d8fdb8-44fa-4c1e-97df-9d4dba14c0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!make\n",
    "timings.add_result(\n",
    "    local_opt.run_problem([\"./mat_mult_transpose_A_patch.exe\",\"-gpu\"], plot=True),\n",
    "    \"Transpose A patch (GPU)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a150e221-5925-478e-8de6-069633469085",
   "metadata": {},
   "outputs": [],
   "source": [
    "timings.plot_results(\"Transpose A patch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6aad73-7e6f-4630-ad67-2dd0e8620a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "timings.add_result(\n",
    "    local_opt.run_problem([\"./mat_mult_patch.exe\",\"-gpu\"], plot=False),\n",
    "    \"Patch (GPU)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bec98f4-2d0e-4e1a-bb8b-4adac6fd13fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_mul_float.check_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaf3e4d-47d2-4f9c-8d9d-4cdb5dcda4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "timings.add_result(\n",
    "    local_opt.run_problem([\"./mat_mult_patch_local.exe\",\"-gpu\"], plot=False),\n",
    "    \"Patch local (GPU)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e494e2a-8bec-423d-a969-64cc24029540",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_mul_float.check_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb97493-2a96-4bc9-a8ba-ce1b45ed2665",
   "metadata": {},
   "outputs": [],
   "source": [
    "!make\n",
    "timings.add_result(\n",
    "    local_opt.run_problem([\"./mat_mult_patch_local_vector.exe\",\"-gpu\"], plot=False),\n",
    "    \"Patch local vector (GPU)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c6b5f6-7f73-4479-8d81-c93a5ca776cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_mul_float.check_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a7a55a-949f-4f95-b800-1b950e773892",
   "metadata": {},
   "outputs": [],
   "source": [
    "!make\n",
    "timings.add_result(\n",
    "    local_opt.run_problem([\"./mat_mult_patch_local_vector.exe\",\"-cpu\"], plot=False),\n",
    "    \"Patch local vector (CPU)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498dcc2f-bd42-44ee-87d0-a1d2dd1b6f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_mul_float.check_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c9d8e6-d99a-478a-802e-19986cc3213e",
   "metadata": {},
   "outputs": [],
   "source": [
    "timings.plot_results(\"Patch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d3c818-bffa-4263-bc20-0c8cd396b172",
   "metadata": {},
   "source": [
    "### Using vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e7c31b-e960-4b5b-8ed3-8dc15e310d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "!make\n",
    "timings.add_result(\n",
    "    local_opt.run_problem([\"./mat_mult_patch_vector.exe\",\"-gpu\"], plot=True),\n",
    "    \"Patch vector (GPU)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f56370e-3b62-4b38-9e5e-c16cddb595f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!make\n",
    "timings.add_result(\n",
    "    local_opt.run_problem([\"./mat_mult_patch_vector.exe\",\"-gpu\"], plot=True),\n",
    "    \"Patch vector prefetch (GPU)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d946bca-a0a9-4b62-9dd8-218a3b225e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "!make\n",
    "timings.add_result(\n",
    "    local_opt.run_problem([\"./mat_mult_patch_vector.exe\",\"-cpu\"], plot=False),\n",
    "    \"Patch vector prefetch (CPU)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55493cde-a195-4e59-8fcc-34dc7fa1cea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_mul_float.check_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b848a5-39cb-4c9c-879c-39af9e568075",
   "metadata": {},
   "source": [
    "### Device partitioning\n",
    "\n",
    "### Using Multiple devices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1bf160-3094-4429-a331-6c957f595d95",
   "metadata": {},
   "source": [
    "## Optismising IO to your compute devices\n",
    "\n",
    "* Shared Virtual Memory\n",
    "* Pinned memory and DMA transfers\n",
    "* Overlapping IO with compute\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6db596-5f45-4318-8366-f673bcf768ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Research papers\n",
    "\n",
    "* [Using OpenCL on CPUs](https://www.hindawi.com/journals/sp/2015/859491/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
